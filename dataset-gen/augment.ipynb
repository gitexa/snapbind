{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1f2bdd37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/xt/01bddk3n7wg7076w11mqj8640000gn/T/ipykernel_18380/3528143824.py:24: DtypeWarning: Columns (67,69,70,71,73,75,76,77,79,81,82,83,85,87,88,89,91,93,94,95,97,99,100,101,121,123,124,125,127,129,130,131,133,135,136,137,139,141,142,143,145,147,148,149,151,153,154,155,157,159,160,161,193,195,196,197,223,225,226,227,229,231,232,233,241,243,244,245,247,249,250,251,253,255,256,257,259,261,262,263,265,267,268,269,271,273,274,275,277,279,280,281,283,285,286,287,289,291,292,293,295,297,298,299,301,303,304,305,307,309,310,311,313,315,316,317,319,321,322,323,325,327,328,329,331,333,334,335,337,339,340,341,343,345,346,347,349,351,352,353,355,357,358,359,361,363,364,365,367,369,370,371,373,375,376,377,379,381,382,383) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  self.df = pd.read_csv(self.csv_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded: 97541 entries\n",
      "Starting augmentation with factor 20...\n",
      "Identifying entries with no binding interactions...\n",
      "Found 1062 entries with no binding interactions\n",
      "Found 96479 entries with binding interactions\n",
      "Augmentation complete:\n",
      "  Original dataset: 97541 entries\n",
      "  Augmented entries: 21240 entries\n",
      "  Final dataset: 118781 entries\n",
      "\n",
      "Validating augmentation...\n",
      "Validation Results:\n",
      "  Total entries: 118781\n",
      "  Original entries: 97541\n",
      "  Augmented entries: 21240\n",
      "  Entries with binding: 96479\n",
      "  Entries without binding: 22302\n",
      "  Binding ratio: 0.812\n",
      "\n",
      "Saving augmented dataset to augmented_protein_dataset.csv...\n",
      "Dataset saved successfully!\n",
      "\n",
      "Augmentation process completed successfully!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "from typing import List, Dict, Tuple\n",
    "import ast\n",
    "import re\n",
    "\n",
    "class ProteinDataAugmenter:\n",
    "    def __init__(self, csv_path: str):\n",
    "        \"\"\"\n",
    "        Initialize the augmenter with the dataset path.\n",
    "        \n",
    "        Args:\n",
    "            csv_path: Path to the merged_protein_dataset_ext.csv file\n",
    "        \"\"\"\n",
    "        self.csv_path = csv_path\n",
    "        self.df = None\n",
    "        self.amino_acids = ['A', 'R', 'N', 'D', 'C', 'Q', 'E', 'G', 'H', 'I', \n",
    "                           'L', 'K', 'M', 'F', 'P', 'S', 'T', 'W', 'Y', 'V']\n",
    "        \n",
    "    def load_data(self):\n",
    "        \"\"\"Load the CSV file into a pandas DataFrame.\"\"\"\n",
    "        print(\"Loading dataset...\")\n",
    "        self.df = pd.read_csv(self.csv_path)\n",
    "        print(f\"Dataset loaded: {len(self.df)} entries\")\n",
    "        return self.df\n",
    "    \n",
    "    def get_chain_columns(self) -> List[str]:\n",
    "        \"\"\"Get all chain sequence columns from the dataset.\"\"\"\n",
    "        sequence_cols = [col for col in self.df.columns if col.endswith('_sequence')]\n",
    "        return sequence_cols\n",
    "    \n",
    "    def get_binding_array_columns(self) -> List[str]:\n",
    "        \"\"\"Get all binding array columns from the dataset.\"\"\"\n",
    "        binding_cols = [col for col in self.df.columns if col.endswith('_binding_array')]\n",
    "        return binding_cols\n",
    "    \n",
    "    def parse_binding_array(self, array_str) -> List[int]:\n",
    "        \"\"\"\n",
    "        Parse binding array string to list of integers.\n",
    "        \n",
    "        Args:\n",
    "            array_str: String representation of binding array\n",
    "            \n",
    "        Returns:\n",
    "            List of integers representing binding positions\n",
    "        \"\"\"\n",
    "        if pd.isna(array_str) or array_str == '':\n",
    "            return []\n",
    "        \n",
    "        try:\n",
    "            # Handle string representation of list\n",
    "            if isinstance(array_str, str):\n",
    "                # Remove brackets and split by comma\n",
    "                array_str = array_str.strip('[]')\n",
    "                if not array_str:\n",
    "                    return []\n",
    "                return [int(x.strip()) for x in array_str.split(',')]\n",
    "            else:\n",
    "                return ast.literal_eval(array_str)\n",
    "        except:\n",
    "            return []\n",
    "    \n",
    "    def has_no_binding(self, row) -> bool:\n",
    "        \"\"\"\n",
    "        Check if a row has no binding interactions (all binding arrays are 0s).\n",
    "        \n",
    "        Args:\n",
    "            row: DataFrame row\n",
    "            \n",
    "        Returns:\n",
    "            True if no binding interactions exist\n",
    "        \"\"\"\n",
    "        binding_cols = self.get_binding_array_columns()\n",
    "        \n",
    "        for col in binding_cols:\n",
    "            if pd.notna(row[col]) and row[col] != '':\n",
    "                binding_array = self.parse_binding_array(row[col])\n",
    "                if binding_array and any(x == 1 for x in binding_array):\n",
    "                    return False\n",
    "        \n",
    "        return True\n",
    "    \n",
    "    def mutate_sequence(self, sequence: str, num_mutations: int) -> str:\n",
    "        \"\"\"\n",
    "        Introduce random point mutations in a protein sequence.\n",
    "        \n",
    "        Args:\n",
    "            sequence: Original protein sequence\n",
    "            num_mutations: Number of mutations to introduce\n",
    "            \n",
    "        Returns:\n",
    "            Mutated protein sequence\n",
    "        \"\"\"\n",
    "        if not sequence or pd.isna(sequence):\n",
    "            return sequence\n",
    "            \n",
    "        sequence_list = list(sequence)\n",
    "        sequence_length = len(sequence_list)\n",
    "        \n",
    "        if sequence_length == 0:\n",
    "            return sequence\n",
    "            \n",
    "        # Randomly select positions to mutate\n",
    "        mutation_positions = random.sample(range(sequence_length), \n",
    "                                         min(num_mutations, sequence_length))\n",
    "        \n",
    "        for pos in mutation_positions:\n",
    "            original_aa = sequence_list[pos]\n",
    "            # Choose a different amino acid\n",
    "            possible_mutations = [aa for aa in self.amino_acids if aa != original_aa]\n",
    "            new_aa = random.choice(possible_mutations)\n",
    "            sequence_list[pos] = new_aa\n",
    "        \n",
    "        return ''.join(sequence_list)\n",
    "    \n",
    "    def augment_row(self, row, augmentation_id: int) -> pd.Series:\n",
    "        \"\"\"\n",
    "        Create an augmented version of a row with mutations.\n",
    "        \n",
    "        Args:\n",
    "            row: Original DataFrame row\n",
    "            augmentation_id: Unique identifier for this augmentation\n",
    "            \n",
    "        Returns:\n",
    "            Augmented row\n",
    "        \"\"\"\n",
    "        new_row = row.copy()\n",
    "        \n",
    "        # Add augmentation identifier to PDB ID\n",
    "        original_pdb = row['pdb_id']\n",
    "        new_row['pdb_id'] = f\"{original_pdb}_aug_{augmentation_id}\"\n",
    "        \n",
    "        # Get sequence columns and introduce mutations\n",
    "        sequence_cols = self.get_chain_columns()\n",
    "        num_mutations = random.randint(1, 10)\n",
    "        \n",
    "        for col in sequence_cols:\n",
    "            if pd.notna(row[col]) and row[col] != '':\n",
    "                new_row[col] = self.mutate_sequence(row[col], num_mutations)\n",
    "        \n",
    "        return new_row\n",
    "    \n",
    "    def identify_no_binding_entries(self) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Identify entries with no binding interactions.\n",
    "        \n",
    "        Returns:\n",
    "            DataFrame containing only no-binding entries\n",
    "        \"\"\"\n",
    "        print(\"Identifying entries with no binding interactions...\")\n",
    "        no_binding_mask = self.df.apply(self.has_no_binding, axis=1)\n",
    "        no_binding_df = self.df[no_binding_mask].copy()\n",
    "        \n",
    "        print(f\"Found {len(no_binding_df)} entries with no binding interactions\")\n",
    "        print(f\"Found {len(self.df) - len(no_binding_df)} entries with binding interactions\")\n",
    "        \n",
    "        return no_binding_df\n",
    "    \n",
    "    def augment_dataset(self, augmentation_factor: int = 2) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Augment the dataset by creating mutated versions of no-binding entries.\n",
    "        \n",
    "        Args:\n",
    "            augmentation_factor: How many augmented versions to create for each no-binding entry\n",
    "            \n",
    "        Returns:\n",
    "            Augmented dataset\n",
    "        \"\"\"\n",
    "        if self.df is None:\n",
    "            raise ValueError(\"Dataset not loaded. Call load_data() first.\")\n",
    "        \n",
    "        print(f\"Starting augmentation with factor {augmentation_factor}...\")\n",
    "        \n",
    "        # Identify no-binding entries\n",
    "        no_binding_df = self.identify_no_binding_entries()\n",
    "        \n",
    "        # Create augmented versions\n",
    "        augmented_rows = []\n",
    "        \n",
    "        for idx, (_, row) in enumerate(no_binding_df.iterrows()):\n",
    "            for aug_num in range(augmentation_factor):\n",
    "                augmentation_id = f\"{idx}_{aug_num}\"\n",
    "                augmented_row = self.augment_row(row, augmentation_id)\n",
    "                augmented_rows.append(augmented_row)\n",
    "        \n",
    "        # Combine original dataset with augmented data\n",
    "        augmented_df = pd.DataFrame(augmented_rows)\n",
    "        final_df = pd.concat([self.df, augmented_df], ignore_index=True)\n",
    "        \n",
    "        print(f\"Augmentation complete:\")\n",
    "        print(f\"  Original dataset: {len(self.df)} entries\")\n",
    "        print(f\"  Augmented entries: {len(augmented_df)} entries\")\n",
    "        print(f\"  Final dataset: {len(final_df)} entries\")\n",
    "        \n",
    "        return final_df\n",
    "    \n",
    "    def validate_augmentation(self, augmented_df: pd.DataFrame) -> Dict[str, int]:\n",
    "        \"\"\"\n",
    "        Validate the augmentation results.\n",
    "        \n",
    "        Args:\n",
    "            augmented_df: The augmented dataset\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary with validation statistics\n",
    "        \"\"\"\n",
    "        print(\"\\nValidating augmentation...\")\n",
    "        \n",
    "        # Count original vs augmented entries\n",
    "        original_entries = len([pdb for pdb in augmented_df['pdb_id'] if '_aug_' not in str(pdb)])\n",
    "        augmented_entries = len([pdb for pdb in augmented_df['pdb_id'] if '_aug_' in str(pdb)])\n",
    "        \n",
    "        # Count binding vs no-binding in final dataset\n",
    "        binding_count = 0\n",
    "        no_binding_count = 0\n",
    "        \n",
    "        for _, row in augmented_df.iterrows():\n",
    "            if self.has_no_binding(row):\n",
    "                no_binding_count += 1\n",
    "            else:\n",
    "                binding_count += 1\n",
    "        \n",
    "        validation_stats = {\n",
    "            'total_entries': len(augmented_df),\n",
    "            'original_entries': original_entries,\n",
    "            'augmented_entries': augmented_entries,\n",
    "            'binding_entries': binding_count,\n",
    "            'no_binding_entries': no_binding_count\n",
    "        }\n",
    "        \n",
    "        print(f\"Validation Results:\")\n",
    "        print(f\"  Total entries: {validation_stats['total_entries']}\")\n",
    "        print(f\"  Original entries: {validation_stats['original_entries']}\")\n",
    "        print(f\"  Augmented entries: {validation_stats['augmented_entries']}\")\n",
    "        print(f\"  Entries with binding: {validation_stats['binding_entries']}\")\n",
    "        print(f\"  Entries without binding: {validation_stats['no_binding_entries']}\")\n",
    "        print(f\"  Binding ratio: {validation_stats['binding_entries']/(validation_stats['binding_entries']+validation_stats['no_binding_entries']):.3f}\")\n",
    "        \n",
    "        return validation_stats\n",
    "    \n",
    "    def save_augmented_dataset(self, augmented_df: pd.DataFrame, output_path: str):\n",
    "        \"\"\"\n",
    "        Save the augmented dataset to a CSV file.\n",
    "        \n",
    "        Args:\n",
    "            augmented_df: The augmented dataset\n",
    "            output_path: Path to save the augmented dataset\n",
    "        \"\"\"\n",
    "        print(f\"\\nSaving augmented dataset to {output_path}...\")\n",
    "        augmented_df.to_csv(output_path, index=False)\n",
    "        print(\"Dataset saved successfully!\")\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main function to run the augmentation process.\"\"\"\n",
    "    \n",
    "    # Configuration\n",
    "    input_file = \"merged_protein_dataset_ext.csv\"\n",
    "    output_file = \"augmented_protein_dataset.csv\"\n",
    "    augmentation_factor = 20  # Adjust this value as needed\n",
    "    \n",
    "    # Initialize augmenter\n",
    "    augmenter = ProteinDataAugmenter(input_file)\n",
    "    \n",
    "    # Load data             \n",
    "    augmenter.load_data()\n",
    "    \n",
    "    # Perform augmentation\n",
    "    augmented_dataset = augmenter.augment_dataset(augmentation_factor=augmentation_factor)\n",
    "    \n",
    "    # Validate results\n",
    "    validation_stats = augmenter.validate_augmentation(augmented_dataset)\n",
    "    \n",
    "    # Save augmented dataset\n",
    "    augmenter.save_augmented_dataset(augmented_dataset, output_file)\n",
    "    \n",
    "    return augmented_dataset, validation_stats\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Set random seed for reproducibility\n",
    "    random.seed(42)\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    # Run augmentation\n",
    "    augmented_data, stats = main()\n",
    "    \n",
    "    print(f\"\\nAugmentation process completed successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30205b48",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7644cbca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
