{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d276f7ac",
   "metadata": {},
   "source": [
    "# ðŸ§¬ Protein Binding Site Prediction with CNNs\n",
    "\n",
    "## Table of Contents\n",
    "- [Overview](#overview)\n",
    "- [Requirements](#requirements)\n",
    "- [Data Preparation](#data-preparation)\n",
    "- [Model Architecture](#model-architecture)\n",
    "- [Training](#training)\n",
    "- [Evaluation](#evaluation)\n",
    "- [Usage Examples](#usage-examples)\n",
    "- [File Structure](#file-structure)\n",
    "- [Troubleshooting](#troubleshooting)\n",
    "\n",
    "## Overview\n",
    "\n",
    "This project implements a deep learning approach for predicting protein-ligand binding sites using Convolutional Neural Networks (CNNs). The model takes protein sequences as input and predicts per-residue binding probabilities.\n",
    "\n",
    "### Key Features\n",
    "- **Multi-scale dilated convolutions** for capturing patterns at different scales\n",
    "- **Residual blocks** for deep feature learning\n",
    "- **Self-attention mechanisms** for long-range dependencies  \n",
    "- **Train/test split** at PDB structure level to prevent data leakage\n",
    "- **Comprehensive evaluation** with visualization tools\n",
    "\n",
    "## Requirements\n",
    "\n",
    "### Dependencies\n",
    "```python\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ast\n",
    "import string\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "```\n",
    "\n",
    "### Hardware Requirements\n",
    "- **Minimum**: 8GB RAM, GPU with 4GB VRAM\n",
    "- **Recommended**: 16GB+ RAM, GPU with 8GB+ VRAM\n",
    "- **Training time**: 2-8 hours depending on dataset size and hardware\n",
    "\n",
    "## Data Preparation\n",
    "\n",
    "### Dataset Format\n",
    "Your CSV file should contain:\n",
    "- `pdb_id`: Protein structure identifier\n",
    "- `ligand_name`: Ligand identifier\n",
    "- `chain_X_sequence`: Amino acid sequence for chain X\n",
    "- `chain_X_binding_array`: Binary array indicating binding sites\n",
    "\n",
    "### Dataset Split Strategy\n",
    "```python\n",
    "# Automatic 90/10 train/test split at PDB level\n",
    "train_dataset = BioLiPDataset(csv_file, split='train')  # 90% of PDB structures\n",
    "test_dataset = BioLiPDataset(csv_file, split='test')   # 10% of PDB structures\n",
    "```\n",
    "\n",
    "### Data Processing\n",
    "- Sequences filtered by length (20-1000 residues)\n",
    "- 22 amino acid vocabulary (20 standard + PAD + UNK)\n",
    "- Binding arrays parsed from string representations\n",
    "- Automatic padding and masking for variable lengths\n",
    "\n",
    "## Model Architecture\n",
    "\n",
    "### FastCNNBindingPredictor\n",
    "```python\n",
    "FastCNNBindingPredictor(\n",
    "    vocab_size=22,      # 20 AA + PAD + UNK\n",
    "    embed_dim=64,       # Embedding dimension\n",
    "    hidden_dim=128,     # Hidden layer size\n",
    "    dropout=0.1         # Dropout rate\n",
    ")\n",
    "```\n",
    "\n",
    "#### Architecture Components:\n",
    "1. **Embedding Layer**: Converts amino acids to dense vectors\n",
    "2. **Multi-scale Convolutions**: \n",
    "   - Kernel sizes: 3, 5 with dilations 1, 2, 4\n",
    "   - Captures local and distant patterns\n",
    "3. **Residual Blocks**: Deep feature learning with skip connections\n",
    "4. **Self-Attention**: Global context modeling\n",
    "5. **Classification Head**: Per-residue binding prediction\n",
    "\n",
    "### Enhanced Model\n",
    "For higher accuracy, use `EnhancedCNNBindingPredictor`:\n",
    "- 4x more parameters (512 hidden dim vs 128)\n",
    "- 8 multi-scale convolution layers\n",
    "- 8 residual blocks with different kernel sizes\n",
    "- Multiple attention heads\n",
    "- Channel attention and squeeze-excitation\n",
    "\n",
    "## Training\n",
    "\n",
    "### Basic Training Setup\n",
    "```python\n",
    "def main():\n",
    "    # Configuration\n",
    "    CSV_FILE = 'merged_protein_dataset.csv'\n",
    "    BATCH_SIZE = 32\n",
    "    MAX_LENGTH = 300\n",
    "    MIN_LENGTH = 20\n",
    "    NUM_EPOCHS = 10\n",
    "    LEARNING_RATE = 2e-4\n",
    "    \n",
    "    # Load training data (90% of PDB structures)\n",
    "    dataset = BioLiPDataset(CSV_FILE, max_length=MAX_LENGTH, \n",
    "                           min_length=MIN_LENGTH, split='train')\n",
    "    \n",
    "    # Create train/validation split\n",
    "    train_dataset, val_dataset = create_train_val_split(dataset)\n",
    "    \n",
    "    # Initialize model\n",
    "    model = FastCNNBindingPredictor(embed_dim=64, hidden_dim=128)\n",
    "    \n",
    "    # Train\n",
    "    train_losses, val_f1_scores = train_model(\n",
    "        train_loader, val_loader, model, device, \n",
    "        num_epochs=NUM_EPOCHS, lr=LEARNING_RATE\n",
    "    )\n",
    "```\n",
    "\n",
    "### Training Features\n",
    "- **Class imbalance handling**: Positive weight calculation\n",
    "- **Early stopping**: Prevents overfitting\n",
    "- **Learning rate scheduling**: Reduces LR on plateau\n",
    "- **Gradient clipping**: Stable training\n",
    "- **Model checkpointing**: Saves best F1 score model\n",
    "\n",
    "### Loss Function\n",
    "```python\n",
    "# Handles class imbalance automatically\n",
    "pos_weight = compute_pos_weight(train_loader)\n",
    "criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight.to(device))\n",
    "```\n",
    "\n",
    "## Evaluation\n",
    "\n",
    "### Test Set Evaluation\n",
    "```python\n",
    "# Evaluate on held-out test set (10% of PDB structures)\n",
    "evaluator, metrics = run_comprehensive_evaluation(\n",
    "    model_path='best_binding_model.pth',\n",
    "    csv_file='merged_protein_dataset.csv',\n",
    "    subset_size=2000\n",
    ")\n",
    "```\n",
    "\n",
    "### Evaluation Metrics\n",
    "- **Overall Performance**: Accuracy, Precision, Recall, F1-Score, AUC\n",
    "- **Probability Distributions**: Separation between binding/non-binding sites\n",
    "- **No-binding Analysis**: False positive rates on sequences without binding sites\n",
    "- **Visualization**: ROC curves, PR curves, probability histograms\n",
    "\n",
    "### Visual Evaluation\n",
    "```python\n",
    "# Create detailed comparison plots\n",
    "results = run_csv_evaluation(\n",
    "    model_path='best_binding_model.pth',\n",
    "    csv_file='merged_protein_dataset.csv',\n",
    "    num_sequences=8,\n",
    "    detailed_plots=True\n",
    ")\n",
    "```\n",
    "\n",
    "Generates:\n",
    "- **Per-sequence plots**: Predicted probabilities vs ground truth\n",
    "- **Overview plots**: Multiple sequences comparison\n",
    "- **Performance metrics**: Per-sequence F1, precision, recall\n",
    "\n",
    "## Usage Examples\n",
    "\n",
    "### 1. Training a New Model\n",
    "```python\n",
    "if __name__ == '__main__':\n",
    "    main()  # Trains model and saves to 'best_binding_model.pth'\n",
    "```\n",
    "\n",
    "### 2. Loading and Testing Model\n",
    "```python\n",
    "# Test on first sequence from test set\n",
    "results = test_model_on_first_sequence(\n",
    "    model_path='best_binding_model.pth',\n",
    "    csv_file='merged_protein_dataset.csv'\n",
    ")\n",
    "\n",
    "# Visualize results\n",
    "fig = visualize_first_test_result(results)\n",
    "```\n",
    "\n",
    "### 3. Predicting Custom Sequence\n",
    "```python\n",
    "# Load model\n",
    "device = torch.device('mps' if torch.mps.is_available() else 'cpu')\n",
    "model = load_trained_model('best_binding_model.pth', device)\n",
    "\n",
    "# Predict\n",
    "sequence = \"MKVLWAALLVTFLAG...\"  # Your protein sequence\n",
    "result = predict_binding_sites(model, sequence, device=device)\n",
    "\n",
    "# Visualize\n",
    "visualize_binding_prediction(result)\n",
    "```\n",
    "\n",
    "### 4. Comprehensive Evaluation\n",
    "```python\n",
    "# Full evaluation on test set\n",
    "evaluator, metrics = run_comprehensive_evaluation(\n",
    "    model_path='best_binding_model.pth',\n",
    "    csv_file='merged_protein_dataset.csv'\n",
    ")\n",
    "\n",
    "# Print detailed report\n",
    "evaluator.print_detailed_report(metrics)\n",
    "\n",
    "# Generate plots\n",
    "evaluator.plot_evaluation_results('evaluation_results.png')\n",
    "```\n",
    "\n",
    "## File Structure\n",
    "\n",
    "```\n",
    "project/\n",
    "â”œâ”€â”€ merged_protein_dataset.csv          # Main dataset\n",
    "â”œâ”€â”€ best_binding_model.pth             # Saved model checkpoint\n",
    "â”œâ”€â”€ training_curves.png                # Training progress plots\n",
    "â”œâ”€â”€ evaluation_plots.png               # Comprehensive evaluation\n",
    "â”œâ”€â”€ detailed_comparison_seq_*.png       # Per-sequence comparisons\n",
    "â””â”€â”€ notebooks/\n",
    "    â”œâ”€â”€ training.ipynb                 # Main training notebook\n",
    "    â”œâ”€â”€ evaluation.ipynb              # Evaluation and testing\n",
    "    â””â”€â”€ inference.ipynb               # Single sequence prediction\n",
    "```\n",
    "\n",
    "## Troubleshooting\n",
    "\n",
    "### Common Issues\n",
    "\n",
    "#### 1. **Memory Errors**\n",
    "```python\n",
    "# Reduce batch size\n",
    "BATCH_SIZE = 16  # Instead of 32\n",
    "\n",
    "# Use smaller model\n",
    "model = FastCNNBindingPredictor(embed_dim=32, hidden_dim=64)\n",
    "```\n",
    "\n",
    "#### 2. **Low F1 Scores**\n",
    "- Check class imbalance: `pos_weight` should be > 1\n",
    "- Increase model capacity: Use `EnhancedFastCNNBindingPredictor`\n",
    "- Lower learning rate: Try `1e-4` instead of `2e-4`\n",
    "- More epochs: Increase to 20-50 epochs\n",
    "\n",
    "#### 3. **Overfitting**\n",
    "- Increase dropout: Try 0.2-0.3\n",
    "- Add weight decay: `weight_decay=1e-3` in optimizer\n",
    "- Reduce model size: Lower `hidden_dim`\n",
    "\n",
    "#### 4. **Data Loading Errors**\n",
    "```python\n",
    "# Check CSV format\n",
    "print(dataset.data.head())\n",
    "print(dataset.data.columns)\n",
    "\n",
    "# Verify sequences are loaded\n",
    "print(f\"Loaded {len(dataset)} sequences\")\n",
    "print(f\"Sample sequence: {dataset[0]['sequence']}\")\n",
    "```\n",
    "\n",
    "#### 5. **Device Compatibility**\n",
    "```python\n",
    "# Auto-detect best device\n",
    "device = torch.device('mps' if torch.mps.is_available() else \n",
    "                     'cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "```\n",
    "\n",
    "### Performance Tips\n",
    "\n",
    "1. **For Better Accuracy**:\n",
    "   - Use `EnhancedFastCNNBindingPredictor`\n",
    "   - Train for more epochs (50-100)\n",
    "   - Use ensemble of multiple models\n",
    "\n",
    "2. **For Faster Training**:\n",
    "   - Increase batch size (if memory allows)\n",
    "   - Use smaller model dimensions\n",
    "   - Sample smaller dataset subset\n",
    "\n",
    "3. **For Debugging**:\n",
    "   - Set `verbose=True` in evaluation functions\n",
    "   - Use `subset_size=100` for quick testing\n",
    "   - Check individual sequence predictions first\n",
    "\n",
    "### Expected Performance\n",
    "\n",
    "| Model | Parameters | F1-Score Range | Training Time |\n",
    "|-------|------------|----------------|---------------|\n",
    "| FastCNN | ~2M | 0.15-0.35 | 1-2 hours |\n",
    "| Enhanced | ~15M | 0.25-0.50 | 4-8 hours |\n",
    "\n",
    "*Performance varies significantly based on dataset quality and class balance.*\n",
    "\n",
    "---\n",
    "\n",
    "## Citation\n",
    "\n",
    "If you use this code, please cite:\n",
    "```bibtex\n",
    "@misc{protein_binding_cnn,\n",
    "  title={Deep Learning for Protein-Ligand Binding Site Prediction},\n",
    "  author={Alex Haas, Elias Bruss, David Barth},\n",
    "  year={2025},\n",
    "  url={https://github.com/gitexa/snapbind}\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "763e315a",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
